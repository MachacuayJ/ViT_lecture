{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      ],
      "metadata": {
        "id": "RGxXhIuCentg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1WtqzFKgO6qSpBGNbbjGMYgWjtprREZfx'>"
      ],
      "metadata": {
        "id": "XEIfhlcEhcRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1wQxQ7BgaLvrRmlFIGU0ymGJ_AmGqYVFM'>"
      ],
      "metadata": {
        "id": "gAEXu6GMmTrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6giDEL1lcHJg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers"
      ],
      "metadata": {
        "id": "YbiWcBwAcBzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Preprocessing"
      ],
      "metadata": {
        "id": "86mZqJIOcPr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize and resize the images."
      ],
      "metadata": {
        "id": "TD8Vg7EPePTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mi2Z1-z8cBU7"
      },
      "outputs": [],
      "source": [
        "class Preprocessing(layers.Layer):\n",
        "  def __init__(self,image_size):\n",
        "    super().__init__()\n",
        "    self.image_size = image_size\n",
        "    self.normalizing_layer = layers.Normalization()\n",
        "    self.resizing_layer = layers.Resizing(image_size,image_size)\n",
        "\n",
        "  def call(self,x):\n",
        "    x = self.normalizing_layer(x)\n",
        "    x = self.resizing_layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Patching"
      ],
      "metadata": {
        "id": "6oJO-HXscUYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the patches from the images."
      ],
      "metadata": {
        "id": "0yrw6buiedHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Patching(layers.Layer):\n",
        "  def __init__(self,patch_size):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self,x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=x,\n",
        "        sizes=[1,self.patch_size,self.patch_size,1],\n",
        "        strides=[1,self.patch_size,self.patch_size,1],\n",
        "        rates=[1,1,1,1],\n",
        "        padding=\"VALID\"\n",
        "    )\n",
        "    patches_dim = tf.shape(patches)[-1]\n",
        "    num_patches = patches.shape[1]*patches.shape[2]\n",
        "    patches = tf.reshape(patches,shape=(batch_size,num_patches,patches_dim))\n",
        "    return patches"
      ],
      "metadata": {
        "id": "4WlZ9rZBcUjG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding Patches"
      ],
      "metadata": {
        "id": "Z6AU2Nkjcgrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the positional embeddings and encode them with the linear projections of the flattened patches."
      ],
      "metadata": {
        "id": "Je6ZAAvRemlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(layers.Layer):\n",
        "  def __init__(self,embedding_dim,num_patches):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_patches = num_patches\n",
        "    self.projection_layer = layers.Dense(embedding_dim)\n",
        "    self.embedding_layer = layers.Embedding(\n",
        "        input_dim = self.num_patches, output_dim=self.embedding_dim\n",
        "    )\n",
        "\n",
        "  def call(self,x):\n",
        "    positions = tf.range(start=0,limit=self.num_patches,delta=1)\n",
        "    embedded_patches = self.projection_layer(x) + self.embedding_layer(positions)\n",
        "    return embedded_patches"
      ],
      "metadata": {
        "id": "Rgb6BQjicg_w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Transformer Encoder*"
      ],
      "metadata": {
        "id": "1aQCBq_tcnFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the transformer encoder. This should follow the following architecture:\n",
        "\n"
      ],
      "metadata": {
        "id": "2Dm8RXXdfNEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1tWq5cjpQUPGHt_MLQRoysmlaQzoEKJpz'>"
      ],
      "metadata": {
        "id": "cOWJBIk3mEo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "  def __init__(self,num_heads,key_dim,value_dim,hidden_neurons,embedding_dim,drop_rate=0.0,eps=1e-08):\n",
        "    super().__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.key_dim = key_dim\n",
        "    self.value_dim = value_dim\n",
        "    self.hidden_neurons = hidden_neurons\n",
        "    self.eps = eps\n",
        "    self.drop_rate = drop_rate\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.norm1 = layers.LayerNormalization(epsilon=self.eps)\n",
        "    self.norm2 = layers.LayerNormalization(epsilon=self.eps)\n",
        "    self.mha = layers.MultiHeadAttention(self.num_heads,self.key_dim,self.value_dim,dropout=self.drop_rate)\n",
        "    self.mlp = tf.keras.models.Sequential([\n",
        "        layers.Dense(self.hidden_neurons, activation=tf.nn.gelu),\n",
        "        layers.Dense(self.embedding_dim, activation=tf.nn.gelu)\n",
        "    ])\n",
        "\n",
        "  def call(self,x):\n",
        "    x1 = self.norm1(x)\n",
        "    x2 = self.mha(x1,x1)\n",
        "    x = layers.Add()([x,x2])\n",
        "    x1 = self.norm2(x)\n",
        "    x2 = self.mlp(x1)\n",
        "    x = layers.Add()([x,x2])\n",
        "    return x"
      ],
      "metadata": {
        "id": "Quma1OAocnqG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "vZFKBjNpczXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "CE7i9JPmdo1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CIFAR100 dataset."
      ],
      "metadata": {
        "id": "8q0ALt17ktw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "input_shape = x_train.shape[1:]"
      ],
      "metadata": {
        "id": "CMDWkig6doaN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build model"
      ],
      "metadata": {
        "id": "dYoi4IsUdqMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the ViT. Do not forget to adapt the normalize layer to the x_train features with the \"adapt\" method of the normalization layer."
      ],
      "metadata": {
        "id": "6iwTJV0dkxTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 64\n",
        "patch_size = 8\n",
        "num_patches = (image_size//patch_size)**2  #This is true under some assumptions: The image size is divisible by the patch size, patches are (patch_size x patch_size), strides are (patch_size x patch_size), and padding=\"VALID\".\n",
        "embedding_dim = 100\n",
        "hidden_neurons = 200\n",
        "key_dim = 1000\n",
        "value_dim = 100\n",
        "num_heads = 8\n",
        "drop_rate = 0.2\n",
        "L = 10  # Num of stacked transformer encoders\n",
        "num_classes = 100\n",
        "\n",
        "input_layer = layers.Input(shape=input_shape)\n",
        "prep_layer = Preprocessing(image_size)\n",
        "prep_layer.normalizing_layer.adapt(x_train)\n",
        "prep_images = prep_layer(input_layer)\n",
        "patches = Patching(patch_size)(prep_images)\n",
        "embeddings = PatchEmbedding(embedding_dim,num_patches)(patches)\n",
        "for _ in range(L):\n",
        "  embeddings = TransformerEncoder(num_heads,key_dim,value_dim,hidden_neurons,embedding_dim,drop_rate)(embeddings)\n",
        "flattened_embeddings = layers.Flatten()(embeddings)\n",
        "hidden_output = layers.Dense(hidden_neurons)(flattened_embeddings)\n",
        "hidden_output = layers.Dropout(drop_rate)(hidden_output)\n",
        "outputs = layers.Dense(num_classes)(hidden_output)\n",
        "model = tf.keras.Model(inputs=input_layer,outputs=outputs)"
      ],
      "metadata": {
        "id": "CJixXfkkcv0F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Vp-msf0iqPtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aab7fd7-54de-4cfe-f6c7-f0e2d2999bff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " preprocessing (Preprocessi  (None, 64, 64, 3)         7         \n",
            " ng)                                                             \n",
            "                                                                 \n",
            " patching (Patching)         (None, 64, 192)           0         \n",
            "                                                                 \n",
            " patch_embedding (PatchEmbe  (None, 64, 100)           25700     \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " transformer_encoder (Trans  (None, 64, 100)           1817600   \n",
            " formerEncoder)                                                  \n",
            "                                                                 \n",
            " transformer_encoder_1 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_2 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_3 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_4 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_5 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_6 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_7 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_8 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " transformer_encoder_9 (Tra  (None, 64, 100)           1817600   \n",
            " nsformerEncoder)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 200)               1280200   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 200)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19502007 (74.39 MB)\n",
            "Trainable params: 19502000 (74.39 MB)\n",
            "Non-trainable params: 7 (32.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model."
      ],
      "metadata": {
        "id": "mNZ6qX0Uk2_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "XCgUZpcek3P7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "jEIYhhVedsPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit the ViT with the CIFAR100 data."
      ],
      "metadata": {
        "id": "sjJ1xfVJk5cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train,y_train,\n",
        "          validation_data=(x_test,y_test),\n",
        "          epochs=10)"
      ],
      "metadata": {
        "id": "cHI9CIxNdr8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fb388f-9799-4d4b-b5e0-46f733a6717e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 404s 235ms/step - loss: 4.2776 - val_loss: 3.3729\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 365s 234ms/step - loss: 3.0963 - val_loss: 3.0956\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 380s 243ms/step - loss: 2.6835 - val_loss: 2.8988\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 365s 234ms/step - loss: 2.3164 - val_loss: 3.0297\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 379s 243ms/step - loss: 1.9800 - val_loss: 3.1501\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 380s 243ms/step - loss: 1.6535 - val_loss: 3.6196\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 366s 234ms/step - loss: 1.4216 - val_loss: 3.9917\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 380s 243ms/step - loss: 1.2736 - val_loss: 4.5171\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 380s 243ms/step - loss: 1.1666 - val_loss: 4.9911\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 381s 244ms/step - loss: 1.1140 - val_loss: 5.5548\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cc2852100a0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z5XqLQPvYUaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}